# -*- coding: utf-8 -*-
"""“shared_bike_data_analysis.ipynb”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uhHrJCkSSUtfeJICaamY43tBioGLbshI

# Shared Bike Data Analysis

Song Beibei

## **1. Upload Dataset**
"""

!pip install googlemaps

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import googlemaps
import seaborn as sns
import io

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#2. Get the file   
downloaded = drive.CreateFile({'id':'1og71Cj0soSvLTasDJY1yMGjXbQksbkJZ'}) # replace the id with id of file you want to access

downloaded.GetContentFile('data.csv')  

#3. Read file as panda dataframe

uploaded = pd.read_csv('data.csv') 
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

uploaded.keys()

"""## **2. Descriptive Analysis and Google Map API**"""

trip = uploaded

trip.head()

trip.isnull().sum()

trip.Duration.describe() # Convert trip duration from second to min as unit

trip.Duration /= 60
trip.head()

trip.Duration.quantile(0.999)

trip.Duration.quantile(0.001)

trip = trip[trip.Duration <= 1252]

trip.head()

trip["Start Station"].describe()

trip["Start Station"].nunique() #70 unique stations

trip["End Station"].nunique()

stations = set(trip["End Station"]).union(trip["Start Station"]) # station exploration

stations

# find geocode for each stations
# via Google Map API
gmaps = googlemaps.Client(key='AIzaSyAbaWq81TxC3PfXnRbTJgq6ID_iA8ekB6I')
geocode ={ }

# by manually checking, restrict the geocode range in California makes it more accurate
bounds = {"southwest" :[37,-123],"northeast":[39,-120]}

geocode

for sta in stations:
    if sta in geocode:
        continue
    try:
        result = gmaps.geocode(sta, bounds = bounds) 
        geocode[sta] = {"lat": result[0]["geometry"]["location"]["lat"], "lng": result[0]["geometry"]["location"]["lng"]}
    except:
        print('Failed to fetch', sta)
        
geocode

geocode['Washington at Kearny']["lat"]

geocode_df = pd.DataFrame(geocode)
geocode_df

# based on the geographical shape of bay area, I sort the stations by their latitude
# so that the stations are more close to each other in geography
geocode_df_transposed = pd.DataFrame.transpose(geocode_df)
station_table = geocode_df_transposed.sort_values("lat")

sorted_station = station_table.index
sorted_station

station_table

"""## 3. Routes and Counts"""

# create a dict with all routes and count
route_count = {i:{j:0 for j in stations} for i in stations}
for i in range(len(trip)):
    try:
        sta = trip.loc[i,"Start Station"]
        end = trip.loc[i,"End Station"]
        route_count[sta][end] += 1
    except:
        continue

trip.loc[100,"Start Station"]

# create a matrix of stations sorted by latitude 
# route_count_df = pd.DataFrame(index = sorted_station, columns = sorted_station)
t = [[route_count[i][j] for j in sorted_station] for i in sorted_station]

route_count_df = pd.DataFrame(np.asarray(t), index = sorted_station, columns = sorted_station)
route_count_df.head()

# Some other descriptive analysis

# Counting function
def counting(lst):
    result = {}
    for i in lst:
        if i not in result:
            result[i] = 1
        else:
            result[i] += 1
    return result 

# Explore trip counts by date 
trip["Start Date"] = pd.to_datetime(trip["Start Date"], format='%m/%d/%Y %H:%M')
trip["End Date"] = pd.to_datetime(trip["End Date"], format='%m/%d/%Y %H:%M')
trip['Date'] = trip["Start Date"].dt.date

# create a date_count dict
dates_count = counting(trip['Date'])

# make date_count a data frame       
df2 = pd.DataFrame.from_dict(dates_count, orient = "index")
df2['date'] = df2.index
df2['trips'] = df2.ix[:,0]
df2 = df2.ix[:,1:3]
df2 = df2.sort_values('date')
df2.reset_index(drop = True, inplace = True)
df2.to_csv("date_count.csv")
df2.head()

"""Next, we could calculate trips count starting and ending from each station, to calculate the in-degree and out-degree centrality of each node (station) in this network graph.

After that, the difference of [`out degree`] - [`in degree`] is calculated. By dividing the average of the two degree, we could get an index the severtiy of bike excess (`diff<0`) or bike shortage (`diff>0`) .
"""

# create a start_station_trip_count dict, as out-degree centrality
start_station_trip_count = counting(trip["Start Station"])
df_start = pd.DataFrame.from_dict(start_station_trip_count, orient='index')
df_start.columns = ["Out Degree Centrality"]

# create an end_station_trip_count dict, as in-degree centrality
end_station_trip_count = counting(trip["End Station"])
df_end = pd.DataFrame.from_dict(end_station_trip_count, orient='index')
df_end.columns = ["In Degree Centrality"]

# combine two dataframes
df_tgt = pd.concat([df_start, df_end], axis=1)
df_tgt["Diff"]  = df_tgt["Out Degree Centrality"] - df_tgt["In Degree Centrality"]
df_tgt["Avg Flow"]  = (df_tgt["Out Degree Centrality"] + df_tgt["In Degree Centrality"])/2
df_tgt["Diff Index"] = df_tgt["Diff"] / df_tgt["Avg Flow"] 
df_tgt.to_csv("station_trip_count.csv")

df_tgt.head()

"""From the data, we construct the above table, where each station has a diff index. 

We could label each station as *'Bike Shortage'* or *'Bike Excess'*. Or we can have further divide each label and based on diff index range to divide them as *'high bike shortage'*,*'moderate bike shortage'* and *'low bike shortage'*.
"""

bike_shortage = df_tgt.sort_values("Diff Index", ascending = False)
bike_shortage.head()

bike_excess= df_tgt.sort_values("Diff Index", ascending = True)
bike_excess.head()

"""For Example, from the above sorted table, we could know the top four stations that has more than 50% of users starting here than ending trip here are indicated as following, where there is **severe bike shortage**:
* Grant Avenue at Columbus Avenue	
* San Francisco City Hall	
* 2nd at Folsom	
* San Jose City Hall

Similary, stations with **severe bike excess** are listed below:
* MLK Library		
* Franklin at Maple	
* San Francisco Caltrain (Townsend at 4th)
* Redwood City Public Library

**Recommendation:**

Firstly, from the result, we could find there are many stations with unbalanced in and out degree of network centrality. A naive suggestion could be re-distribute the supply of bikes by Uber company between stations with bike excess and bike shortage that are geographically close to each other.(e.g. Sending bikes from San Francisco Caltrain station to San Francisco City Hall)

Based on the analysis, I would like to propose a change in pricing model, to use a dynamic algorithm to calculate fee variation between different stations. Similar to current Uber pricing algorithm, where we increase the price in a 'busy' area dynamically, we can make it such that when the user is travelling from a 'Bike Excess' station to a 'Bike Shortage' station, they will receive a discount; they will see a price increase if they travel in the opposite way. One difference between this new algorithm and the existing Uber (car) pricing model is that this shared bike one should not rely solely on real-time data and consider both starting and ending point. 

The current algorithm only considers the starting point, but the new dynamic algorithm should also takes ending point into consideration, in order to encourage users to spontaneously 're-balance' the bike distributions.

## 4. Visualization
"""

# Draw routes matrix in heatmap
# Explore clusterings showing in geographical level
# Here I am not choose any clustering algorithm like kmeans etc. 
# But just to draw a graph to visualize the trips between sorted stations(by latitude)

# normalize route count by using logarithm
df = np.log(route_count_df + 1) # +1 to make it a positive number
plt.subplots(figsize=(20,15))
sns.heatmap(df,cmap="Blues")

"""From the graph, clearly we can find patterns showing in three clusters. 
The top left orange shape are trips within San Jose, and bottom right rectangle indicates the trips are frequently happening within San Francisco. There are a series of small clusters/rectangles in between, which are trips within some small towns in Bay Area, i.e. Redwood, Mountain View and Sunnyvale etc.  

We could discover a general trend reflected from this dataset, that people are more likely to use shared bike for trips within an area(Intra-county travel), not for far distance trips like from San Francisco to San Jose(Inter-county travel).

Besides, from this graph, we could spot some bad points with wrong GOOGLE API Geocode crawling data.

For example: 
1. Japan town are supposed to be in the first cluster with San Jose, however it is now situated in the cluster of San Francisco. It means in the Google API request, it crawls geocode from San Francisco Japan Town, but actually it is the San Jose Japan Town.
2. Similary, for Franklin at Maple and Park at olive, it should be near Palo Alto, and the current geocode is far from correct one.

This graph helps me to identify these points with wrong data from google, and I could manually change these points to correct them.
"""

# correct the bad points

# japan town
# --> should be japantown in san jose instead of japan town in san francisco
station_table.set_value('Japantown', 'lat', 37.348859)
station_table.set_value('Japantown', 'lng', -121.894212)

#Broadway at Main
station_table.set_value('Broadway at Main', 'lat', 37.714567)
station_table.set_value('Broadway at Main', 'lng', -120.193794)

# Park at Olive
# --> should be olive avenue&park blvd in palo alto, instead of current result at Santa Rosa.
station_table.set_value('Park at Olive', 'lat', 37.4254018)
station_table.set_value('Park at Olive', 'lng', -122.1395898)

station_table.set_value('Palo Alto Caltrain Station', 'lat', 37.443742)
station_table.set_value('Palo Alto Caltrain Station', 'lng', -122.165845)

station_table.set_value('Redwood City Caltrain Station', 'lat', 37.485882)
station_table.set_value('Redwood City Caltrain Station', 'lng', -122.230881)

station_table.set_value('Franklin at Maple', 'lat', 37.481733)
station_table.set_value('Franklin at Maple', 'lng', -122.226524)

station_table_revised = station_table.sort_values("lat")
sorted_station_revised = station_table_revised.index
sorted_station_revised

# redraw heatmap to confirm
t_revised = [[route_count[i][j] for j in sorted_station_revised] for i in sorted_station_revised]
route_count_df_revised = pd.DataFrame(np.asarray(t_revised), index = sorted_station_revised, columns = sorted_station_revised)

df_revised = np.log(route_count_df_revised + 1)
plt.subplots(figsize=(20,15))
sns.heatmap(df_revised,cmap="Blues")

"""Now that the stations and clusters are corrected and cleaned, which could be verified by graph.
The route_count matrix between stations is a sparse matrix, with more frequent intra region trips but less inter region trips.
"""

# now we have station-wise trip counts and station geocodes. 
# we output the modified version to csv for visualization purpose
route_count_df_revised.to_csv("route_count.csv")
station_table_revised.to_csv("geocode.csv")